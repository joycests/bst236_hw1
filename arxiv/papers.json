{
  "query": "machine learning",
  "updated_at": "2026-02-19T04:53:08.216529",
  "papers": [
    {
      "title": "One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation",
      "authors": [
        "Zhenyu Wei",
        "Yunchao Yao",
        "Mingyu Ding"
      ],
      "abstract": "Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.",
      "pdf": "https://arxiv.org/pdf/2602.16712v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16712v1",
      "abs_url": "https://arxiv.org/abs/2602.16712v1",
      "published": "2026-02-18"
    },
    {
      "title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data",
      "authors": [
        "Ruijie Zheng",
        "Dantong Niu",
        "Yuqi Xie",
        "Jing Wang",
        "Mengda Xu",
        "Yunfan Jiang",
        "Fernando Casta\u00f1eda",
        "Fengyuan Hu",
        "You Liang Tan",
        "Letian Fu",
        "Trevor Darrell",
        "Furong Huang",
        "Yuke Zhu",
        "Danfei Xu",
        "Linxi Fan"
      ],
      "abstract": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.",
      "pdf": "https://arxiv.org/pdf/2602.16710v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16710v1",
      "abs_url": "https://arxiv.org/abs/2602.16710v1",
      "published": "2026-02-18"
    },
    {
      "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning",
      "authors": [
        "Weijing Tang",
        "Ming Yuan",
        "Zongqi Xia",
        "Tianxi Cai"
      ],
      "abstract": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.",
      "pdf": "https://arxiv.org/pdf/2602.16709v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16709v1",
      "abs_url": "https://arxiv.org/abs/2602.16709v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "authors": [
        "Runpei Dong",
        "Ziyan Li",
        "Xialin He",
        "Saurabh Gupta"
      ],
      "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "pdf": "https://arxiv.org/pdf/2602.16705v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16705v1",
      "abs_url": "https://arxiv.org/abs/2602.16705v1",
      "published": "2026-02-18"
    },
    {
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "authors": [
        "Hee Seung Hwang",
        "Xindi Wu",
        "Sanghyuk Chun",
        "Olga Russakovsky"
      ],
      "abstract": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "pdf": "https://arxiv.org/pdf/2602.16704v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16704v1",
      "abs_url": "https://arxiv.org/abs/2602.16704v1",
      "published": "2026-02-18"
    },
    {
      "title": "The Role of Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems",
      "authors": [
        "Shreya Meel",
        "Sennur Ulukus"
      ],
      "abstract": "In symmetric private information retrieval (SPIR), a user communicates with multiple servers to retrieve from them a message in a database, while not revealing the message index to any individual server (user privacy), and learning no additional information about the database (database privacy). We study the problem of SPIR on graph-replicated database systems, where each node of the graph represents a server and each link represents a message. Each message is replicated at exactly two servers; those at which the link representing the message is incident. To ensure database privacy, the servers share a set of common randomness, independent of the database and the user's desired message index. We study two cases of common randomness distribution to the servers: i) graph-replicated common randomness, and ii) fully-replicated common randomness. Given a graph-replicated database system, in i), we assign one randomness variable independently to every pair of servers sharing a message, while in ii), we assign an identical set of randomness variable to all servers, irrespective of the underlying graph. In both settings, our goal is to characterize the SPIR capacity, i.e., the maximum number of desired message symbols retrieved per downloaded symbol, and quantify the minimum amount of common randomness required to achieve the capacity. To this goal, in setting i), we derive a general lower bound on the SPIR capacity, and show it to be tight for path and regular graphs through a matching converse. Moreover, we establish that the minimum size of common randomness required for SPIR is equal to the message size. In setting ii), the SPIR capacity improves over the first, more restrictive setting. We show this through capacity lower bounds for a class of graphs, by constructing SPIR schemes from PIR schemes.",
      "pdf": "https://arxiv.org/pdf/2602.16700v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16700v1",
      "abs_url": "https://arxiv.org/abs/2602.16700v1",
      "published": "2026-02-18"
    },
    {
      "title": "Causality is Key for Interpretability Claims to Generalise",
      "authors": [
        "Shruti Joshi",
        "Aaron Mueller",
        "David Klindt",
        "Wieland Brendel",
        "Patrik Reizinger",
        "Dhanya Sridhar"
      ],
      "abstract": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
      "pdf": "https://arxiv.org/pdf/2602.16698v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16698v1",
      "abs_url": "https://arxiv.org/abs/2602.16698v1",
      "published": "2026-02-18"
    },
    {
      "title": "Protecting the Undeleted in Machine Unlearning",
      "authors": [
        "Aloni Cohen",
        "Refael Kohen",
        "Kobbi Nissim",
        "Uri Stemmer"
      ],
      "abstract": "Machine unlearning aims to remove specific data points from a trained model, often striving to emulate \"perfect retraining\", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $\u03c9(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.",
      "pdf": "https://arxiv.org/pdf/2602.16697v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16697v1",
      "abs_url": "https://arxiv.org/abs/2602.16697v1",
      "published": "2026-02-18"
    },
    {
      "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
      "authors": [
        "Huan Souza",
        "Pankaj Mehta"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
      "pdf": "https://arxiv.org/pdf/2602.16696v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16696v1",
      "abs_url": "https://arxiv.org/abs/2602.16696v1",
      "published": "2026-02-18"
    },
    {
      "title": "Synthetic-Powered Multiple Testing with FDR Control",
      "authors": [
        "Yonghoon Lee",
        "Meshi Bashari",
        "Edgar Dobriban",
        "Yaniv Romano"
      ],
      "abstract": "Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.",
      "pdf": "https://arxiv.org/pdf/2602.16690v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16690v1",
      "abs_url": "https://arxiv.org/abs/2602.16690v1",
      "published": "2026-02-18"
    },
    {
      "title": "Are Object-Centric Representations Better At Compositional Generalization?",
      "authors": [
        "Ferdinand Kapl",
        "Amir Mohammad Karimi Mamaghan",
        "Maximilian Seitzer",
        "Karl Henrik Johansson",
        "Carsten Marr",
        "Stefan Bauer",
        "Andrea Dittadi"
      ],
      "abstract": "Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.",
      "pdf": "https://arxiv.org/pdf/2602.16689v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16689v1",
      "abs_url": "https://arxiv.org/abs/2602.16689v1",
      "published": "2026-02-18"
    },
    {
      "title": "On the Hardness of Approximation of the Fair k-Center Problem",
      "authors": [
        "Suhas Thejaswi"
      ],
      "abstract": "In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $\u03b5>0$, achieving a $(3-\u03b5)$-approximation is NP-hard, assuming $\\text{P} \\neq \\text{NP}$.\n  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.",
      "pdf": "https://arxiv.org/pdf/2602.16688v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16688v1",
      "abs_url": "https://arxiv.org/abs/2602.16688v1",
      "published": "2026-02-18"
    },
    {
      "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition",
      "authors": [
        "Bo Pan",
        "Peter Zhiping Zhang",
        "Hao-Wei Pang",
        "Alex Zhu",
        "Xiang Yu",
        "Liying Zhang",
        "Liang Zhao"
      ],
      "abstract": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.",
      "pdf": "https://arxiv.org/pdf/2602.16684v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16684v1",
      "abs_url": "https://arxiv.org/abs/2602.16684v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning Situated Awareness in the Real World",
      "authors": [
        "Chuhan Li",
        "Ruilin Han",
        "Joy Hsu",
        "Yongyuan Liang",
        "Rajiv Dhawan",
        "Jiajun Wu",
        "Ming-Hsuan Yang",
        "Xin Eric Wang"
      ],
      "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "pdf": "https://arxiv.org/pdf/2602.16682v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16682v1",
      "abs_url": "https://arxiv.org/abs/2602.16682v1",
      "published": "2026-02-18"
    },
    {
      "title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection",
      "authors": [
        "Yingyuan Yang",
        "Tian Lan",
        "Yifei Gao",
        "Yimeng Lu",
        "Wenjun He",
        "Meng Wang",
        "Chenghao Liu",
        "Chen Zhang"
      ],
      "abstract": "Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.",
      "pdf": "https://arxiv.org/pdf/2602.16681v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16681v1",
      "abs_url": "https://arxiv.org/abs/2602.16681v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation",
      "authors": [
        "Jack Rome",
        "Stephen James",
        "Subramanian Ramamoorthy"
      ],
      "abstract": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.",
      "pdf": "https://arxiv.org/pdf/2602.16675v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16675v1",
      "abs_url": "https://arxiv.org/abs/2602.16675v1",
      "published": "2026-02-18"
    },
    {
      "title": "Neighborhood Stability as a Measure of Nearest Neighbor Searchability",
      "authors": [
        "Thomas Vecchiato",
        "Sebastian Bruch"
      ],
      "abstract": "Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call \"searchability.\" To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.",
      "pdf": "https://arxiv.org/pdf/2602.16673v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16673v1",
      "abs_url": "https://arxiv.org/abs/2602.16673v1",
      "published": "2026-02-18"
    },
    {
      "title": "Towards a Science of AI Agent Reliability",
      "authors": [
        "Stephan Rabanser",
        "Sayash Kapoor",
        "Peter Kirgis",
        "Kangheng Liu",
        "Saiteja Utpala",
        "Arvind Narayanan"
      ],
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "pdf": "https://arxiv.org/pdf/2602.16666v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16666v1",
      "abs_url": "https://arxiv.org/abs/2602.16666v1",
      "published": "2026-02-18"
    },
    {
      "title": "Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning",
      "authors": [
        "Li Zeng",
        "Mutian Shen",
        "Tianle Pu",
        "Zohar Nussinov",
        "Qing Feng",
        "Chao Chen",
        "Zhong Liu",
        "Changjun Fan"
      ],
      "abstract": "p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p>2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.",
      "pdf": "https://arxiv.org/pdf/2602.16665v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16665v1",
      "abs_url": "https://arxiv.org/abs/2602.16665v1",
      "published": "2026-02-18"
    },
    {
      "title": "Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge",
      "authors": [
        "Jiaming Liu",
        "Felix Petersen",
        "Yunhe Gao",
        "Yabin Zhang",
        "Hyojin Kim",
        "Akshay S. Chaudhari",
        "Yu Sun",
        "Stefano Ermon",
        "Sergios Gatidis"
      ],
      "abstract": "Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.",
      "pdf": "https://arxiv.org/pdf/2602.16664v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16664v1",
      "abs_url": "https://arxiv.org/abs/2602.16664v1",
      "published": "2026-02-18"
    },
    {
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "pdf": "https://arxiv.org/pdf/2602.16660v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16660v1",
      "abs_url": "https://arxiv.org/abs/2602.16660v1",
      "published": "2026-02-18"
    },
    {
      "title": "Investigating Nonlinear Quenching Effects on Polar Field Buildup in the Sun Using Physics-Informed Neural Networks",
      "authors": [
        "Jithu J. Athalathil",
        "Mohammed H. Talafha",
        "Bhargav Vaidya"
      ],
      "abstract": "The solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (TQ) and latitude quenching (LQ). These mechanisms play a decisive role in regulating the buildup of the Sun's polar field and, in turn, the amplitude of future solar cycles. In this work, we employ Physics-Informed Neural Networks (PINN) to solve the surface flux transport (SFT) equation, embedding physical constraints directly into the neural network framework. By systematically varying transport parameters, we isolate the relative contributions of TQ and LQ to polar dipole buildup. We use the residual dipole moment as a diagnostic for cycle-to-cycle amplification and show that TQ suppression strengthens with increasing diffusivity, while LQ dominates in advection-dominated regimes. The ratio $\u0394D_{\\mathrm{LQ}}/\u0394D_{\\mathrm{TQ}}$ exhibits a smooth inverse-square dependence on the dynamo effectivity range, refining previous empirical fits with improved accuracy and reduced scatter. The results further reveal that the need for a decay term is not essential for PINN set-up due to the training process. Compared with the traditional 1D SFT model, the PINN framework achieves significantly lower error metrics and more robust recovery of nonlinear trends. Our results suggest that the nonlinear interplay between LQ and TQ can naturally produce alternations between weak and strong cycles, providing a physical explanation for the observed even-odd cycle modulation. These findings demonstrate the potential of PINN as an accurate, efficient, and physically consistent tool for solar cycle prediction.",
      "pdf": "https://arxiv.org/pdf/2602.16656v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16656v1",
      "abs_url": "https://arxiv.org/abs/2602.16656v1",
      "published": "2026-02-18"
    },
    {
      "title": "Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment",
      "authors": [
        "Shuta Kikuchi",
        "Shu Tanaka"
      ],
      "abstract": "The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.",
      "pdf": "https://arxiv.org/pdf/2602.16643v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16643v1",
      "abs_url": "https://arxiv.org/abs/2602.16643v1",
      "published": "2026-02-18"
    },
    {
      "title": "Optimizer choice matters for the emergence of Neural Collapse",
      "authors": [
        "Jim Zhao",
        "Tin Sum Cheng",
        "Wojciech Masarczyk",
        "Aurelien Lucchi"
      ],
      "abstract": "Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.",
      "pdf": "https://arxiv.org/pdf/2602.16642v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16642v1",
      "abs_url": "https://arxiv.org/abs/2602.16642v1",
      "published": "2026-02-18"
    },
    {
      "title": "Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements",
      "authors": [
        "De-Ming Chian",
        "Chao-Kai Wen",
        "Feng-Ji Chen",
        "Yi-Jie Sun",
        "Fu-Kang Wang"
      ],
      "abstract": "We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of M\u00f6bius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported.",
      "pdf": "https://arxiv.org/pdf/2602.16637v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16637v1",
      "abs_url": "https://arxiv.org/abs/2602.16637v1",
      "published": "2026-02-18"
    },
    {
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "authors": [
        "Yu Xie",
        "Ludwig Winkler",
        "Lixin Sun",
        "Sarah Lewis",
        "Adam E. Foster",
        "Jos\u00e9 Jim\u00e9nez Luna",
        "Tim Hempel",
        "Michael Gastegger",
        "Yaoyi Chen",
        "Iryna Zaporozhets",
        "Cecilia Clementi",
        "Christopher M. Bishop",
        "Frank No\u00e9"
      ],
      "abstract": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $\u0394$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.",
      "pdf": "https://arxiv.org/pdf/2602.16634v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16634v1",
      "abs_url": "https://arxiv.org/abs/2602.16634v1",
      "published": "2026-02-18"
    },
    {
      "title": "Behavioral change models for infectious disease transmission: a systematic review (2020-2025)",
      "authors": [
        "Youngji Jo",
        "Sileshi Sintayehu Sharbayta",
        "Bruno Buonomo"
      ],
      "abstract": "Background: Human behavior shapes infectious disease dynamics, yet its integration into transmission models remains fragmented. Recent epidemics, particularly COVID-19, highlight the need for models capturing adaptation to perceived risk, social influence, and policy signals. This review synthesizes post-2020 models incorporating behavioral adaptation, examines their theoretical grounding, and evaluates how behavioral constructs modify transmission, vaccination, and compliance. Methods: Following PRISMA guidelines, we searched Scopus and PubMed (2020-2025), screening 1,274 records with citation chaining. We extracted data on disease context, country, modeling framework, behavioral mechanisms (prevalence-dependent, policy/media, imitation/social learning), and psychosocial constructs (personal threat, coping appraisal, barriers, social norms, cues to action). A total of 216 studies met inclusion criteria. Results: COVID-19 accounted for 73% of studies. Most used compartmental ODE models (81%) and focused on theoretical or U.S. settings. Behavioral change was mainly reactive: 47% applied prevalence-dependent feedback, 25% included awareness/media dynamics, and 19% relied on exogenous policy triggers. Game-theoretic or social learning approaches were rare (less or equal than 5%). Behavioral effects primarily modified contact or transmission rates (91%). Psychosocial constructs were unevenly represented: cues to action (n=159) and personal threat (n=145) dominated, whereas coping appraisal (n=82), barriers (n=36), and social norms (n=25) were less common. Conclusions: We propose a taxonomy structured by behavioral drivers, social scale, and memory to clarify dominant paradigms and their empirical basis. Mapping models to psychosocial constructs provides guidance for more theory-informed and data grounded-integration of behavioral processes in epidemiological modeling.",
      "pdf": "https://arxiv.org/pdf/2602.16633v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16633v1",
      "abs_url": "https://arxiv.org/abs/2602.16633v1",
      "published": "2026-02-18"
    },
    {
      "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes",
      "authors": [
        "Ethan Blaser",
        "Jiuqi Wang",
        "Shangtong Zhang"
      ],
      "abstract": "The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.",
      "pdf": "https://arxiv.org/pdf/2602.16629v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16629v1",
      "abs_url": "https://arxiv.org/abs/2602.16629v1",
      "published": "2026-02-18"
    },
    {
      "title": "Stoichiometry Dependent Properties of Cerium Hydride: An Active Learning Developed Interatomic Potential Study",
      "authors": [
        "Brenden W. Hamilton",
        "Travis E. Jones",
        "Timothy C. Germann",
        "Benjamin T. Nebgen"
      ],
      "abstract": "Cerium hydride has a variety of interesting properties, including a known lattice contraction and densification with increasing hydrogen content. However, precise stoichiometric control is not experimentally straightforward and {\\it ab initio} approaches are not computationally feasible for many properties such as melting and low temperature diffusion. Therefore, we develop a machine-learned interatomic potential for cerium hydride that is valid for H to Ce ratios from 2.0 to 3.0. A query-by-committee active learning approach is used to develop the training set. Leveraging classical molecular dynamics simulations, we assess a range of properties and provide fundamental mechanisms for the trends with stoichiometry. A majority of the properties follow the trend of lattice contraction, being governed by the stronger lattice binding induced by adding octahedral atoms.",
      "pdf": "https://arxiv.org/pdf/2602.16628v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16628v1",
      "abs_url": "https://arxiv.org/abs/2602.16628v1",
      "published": "2026-02-18"
    },
    {
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "authors": [
        "SungJun Cho",
        "Chetan Gohil",
        "Rukuang Huang",
        "Oiwi Parker Jones",
        "Mark W. Woolrich"
      ],
      "abstract": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
      "pdf": "https://arxiv.org/pdf/2602.16626v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16626v1",
      "abs_url": "https://arxiv.org/abs/2602.16626v1",
      "published": "2026-02-18"
    },
    {
      "title": "Beyond the Classical Ceiling: Multi-Layer Fully-Connected Variational Quantum Circuits",
      "authors": [
        "Howard Su",
        "Chen-Yu Liu",
        "Samuel Yen-Chi Chen",
        "Kuan-Cheng Chen",
        "Huan-Hsin Tseng"
      ],
      "abstract": "Standard Variational Quantum Circuits (VQCs) struggle to scale to high-dimensional data due to the ``curse of dimensionality,'' which manifests as exponential simulation costs ($\\mathcal{O}(2^d)$) and untrainable Barren Plateaus. Existing solutions often bypass this by relying on classical neural networks for feature compression, obscuring the true quantum capability. In this work, we propose the \\textbf{Multi-Layer Fully-Connected VQC (FC-VQC)}, a modular architecture that performs \\textbf{end-to-end quantum learning} without trainable classical encoders. By restricting local Hilbert space dimensions while enabling global feature interaction via structured block mixing, our framework achieves \\textbf{linear scalability $\\mathcal{O}(d)$}. We empirically validate this approach on standard benchmarks and a high-dimensional industrial task: \\textbf{300-asset Option Portfolio Pricing}. In this regime, the FC-VQC breaks the ``Classical Ceiling,'' outperforming state-of-the-art Gradient Boosting baselines (XGBoost/CatBoost) while exhibiting \\textbf{$\\approx 17\\times$ greater parameter efficiency} than Deep Neural Networks. These results provide concrete evidence that pure, modular quantum architectures can effectively learn industrial-scale feature spaces that are intractable for monolithic ansatzes.",
      "pdf": "https://arxiv.org/pdf/2602.16623v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16623v1",
      "abs_url": "https://arxiv.org/abs/2602.16623v1",
      "published": "2026-02-18"
    },
    {
      "title": "Addressing Ill-conditioning in Density Functional Theory for Reliable Machine Learning",
      "authors": [
        "L. Arnstein",
        "J. Wetherell",
        "R. Lawrence",
        "P. J. Hasnip",
        "M. J. P. Hodgson"
      ],
      "abstract": "In principle, machine learning (ML) can be used to obtain any electronic property of a many-body system from its electron density within density functional theory. However, some physical quantities are highly sensitive to small variations in the density. This 'ill-conditioning' limits the accuracy with which these quantities can be learned as density functionals from a fixed amount of data. We identify sources of ill-conditioning present in density functionals that belong to two ubiquitous classes: 1) Physical quantities that are globally gauge-dependent, meaning they change value if a constant shift is applied to the external potential -- for example, the total energy; 2) Functionals of the N-electron density that have an implicit dependence on the (N+1)-electron density, such as the fundamental gap. We demonstrate that widely used ML models exhibit orders-of-magnitude greater error when applied to these ill-conditioned density functionals compared to other functionals that fall into neither class, even when the global gauge is fixed to prevent constant shifts. Owing to an absence of ill-conditioning in potential functionals, we find that providing the external potential as input to the ML model leads to significantly improved predictions of quantities in these two classes.",
      "pdf": "https://arxiv.org/pdf/2602.16618v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16618v1",
      "abs_url": "https://arxiv.org/abs/2602.16618v1",
      "published": "2026-02-18"
    },
    {
      "title": "Style-Aware Gloss Control for Generative Non-Photorealistic Rendering",
      "authors": [
        "Santiago Jimenez-Navarro",
        "Belen Masia",
        "Ana Serrano"
      ],
      "abstract": "Humans can infer material characteristics of objects from their visual appearance, and this ability extends to artistic depictions, where similar perceptual strategies guide the interpretation of paintings or drawings. Among the factors that define material appearance, gloss, along with color, is widely regarded as one of the most important, and recent studies indicate that humans can perceive gloss independently of the artistic style used to depict an object. To investigate how gloss and artistic style are represented in learned models, we train an unsupervised generative model on a newly curated dataset of painterly objects designed to systematically vary such factors. Our analysis reveals a hierarchical latent space in which gloss is disentangled from other appearance factors, allowing for a detailed study of how gloss is represented and varies across artistic styles. Building on this representation, we introduce a lightweight adapter that connects our style- and gloss-aware latent space to a latent-diffusion model, enabling the synthesis of non-photorealistic images with fine-grained control of these factors. We compare our approach with previous models and observe improved disentanglement and controllability of the learned factors.",
      "pdf": "https://arxiv.org/pdf/2602.16611v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16611v1",
      "abs_url": "https://arxiv.org/abs/2602.16611v1",
      "published": "2026-02-18"
    },
    {
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "authors": [
        "Mengjie Qian",
        "Guangzhi Sun",
        "Mark J. F. Gales",
        "Kate M. Knill"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "pdf": "https://arxiv.org/pdf/2602.16610v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16610v1",
      "abs_url": "https://arxiv.org/abs/2602.16610v1",
      "published": "2026-02-18"
    },
    {
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "authors": [
        "Melkamu Abay Mersha",
        "Jugal Kalita"
      ],
      "abstract": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "pdf": "https://arxiv.org/pdf/2602.16608v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16608v1",
      "abs_url": "https://arxiv.org/abs/2602.16608v1",
      "published": "2026-02-18"
    },
    {
      "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
      "authors": [
        "Nail B. Khelifa",
        "Richard E. Turner",
        "Ramji Venkataramanan"
      ],
      "abstract": "Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.",
      "pdf": "https://arxiv.org/pdf/2602.16601v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16601v1",
      "abs_url": "https://arxiv.org/abs/2602.16601v1",
      "published": "2026-02-18"
    },
    {
      "title": "Predicting The Cop Number Using Machine Learning",
      "authors": [
        "Meagan Mann",
        "Christian Muise",
        "Erin Meger"
      ],
      "abstract": "Cops and Robbers is a pursuit evasion game played on a graph, first introduced independently by Quilliot \\cite{quilliot1978jeux} and Nowakowski and Winkler \\cite{NOWAKOWSKI1983235} over four decades ago. A main interest in recent the literature is identifying the cop number of graph families. The cop number of a graph, $c(G)$, is defined as the minimum number of cops required to guarantee capture of the robber. Determining the cop number is computationally difficult and exact algorithms for this are typically restricted to small graph families. This paper investigates whether classical machine learning methods and graph neural networks can accurately predict a graph's cop number from its structural properties and identify which properties most strongly influence this prediction. Of the classical machine learning models, tree-based models achieve high accuracy in prediction despite class imbalance, whereas graph neural networks achieve comparable results without explicit feature engineering. The interpretability analysis shows that the most predictive features are related to node connectivity, clustering, clique structure, and width parameters, which aligns with known theoretical results. Our findings suggest that machine learning approaches can be used in complement with existing cop number algorithms by offering scalable approximations where computation is infeasible.",
      "pdf": "https://arxiv.org/pdf/2602.16600v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16600v1",
      "abs_url": "https://arxiv.org/abs/2602.16600v1",
      "published": "2026-02-18"
    },
    {
      "title": "Sequential Membership Inference Attacks",
      "authors": [
        "Thomas Michel",
        "Debabrota Basu",
        "Emilie Kaufmann"
      ],
      "abstract": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "pdf": "https://arxiv.org/pdf/2602.16596v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16596v1",
      "abs_url": "https://arxiv.org/abs/2602.16596v1",
      "published": "2026-02-18"
    },
    {
      "title": "Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles",
      "authors": [
        "Abhishek Goudar",
        "Angela P. Schoellig"
      ],
      "abstract": "Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.",
      "pdf": "https://arxiv.org/pdf/2602.16594v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16594v1",
      "abs_url": "https://arxiv.org/abs/2602.16594v1",
      "published": "2026-02-18"
    },
    {
      "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
      "authors": [
        "Qi You",
        "Yitai Cheng",
        "Zichao Zeng",
        "James Haworth"
      ],
      "abstract": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
      "pdf": "https://arxiv.org/pdf/2602.16590v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16590v1",
      "abs_url": "https://arxiv.org/abs/2602.16590v1",
      "published": "2026-02-18"
    },
    {
      "title": "Nonparametric Kernel Regression for Coordinated Energy Storage Peak Shaving with Stacked Services",
      "authors": [
        "Emily Logan",
        "Ning Qi",
        "Bolun Xu"
      ],
      "abstract": "Developing effective control strategies for behind-the-meter energy storage to coordinate peak shaving and stacked services is essential for reducing electricity costs and extending battery lifetime in commercial buildings. This work proposes an end-to-end, two-stage framework for coordinating peak shaving and energy arbitrage with a theoretical decomposition guarantee. In the first stage, a non-parametric kernel regression model constructs state-of-charge trajectory bounds from historical data that satisfy peak-shaving requirements. The second stage utilizes the remaining capacity for energy arbitrage via a transfer learning method. Case studies using New York City commercial building demand data show that our method achieves a 1.3 times improvement in performance over the state-of-the-art forecast-based method, achieving cost savings and effective peak management without relying on predictions.",
      "pdf": "https://arxiv.org/pdf/2602.16586v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16586v1",
      "abs_url": "https://arxiv.org/abs/2602.16586v1",
      "published": "2026-02-18"
    },
    {
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "abstract": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "pdf": "https://arxiv.org/pdf/2602.16585v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16585v1",
      "abs_url": "https://arxiv.org/abs/2602.16585v1",
      "published": "2026-02-18"
    },
    {
      "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
      "authors": [
        "Akhil Ramidi",
        "Kevin Scharp"
      ],
      "abstract": "There is growing evidence that independently trained AI systems come to represent the world in the same way. In other words, independently trained embeddings from text, vision, audio, and neural signals share an underlying geometry. We call this the Representational Alignment Hypothesis (RAH) and investigate evidence for and consequences of this claim. The evidence is of two kinds: (i) internal structure comparison techniques, such as representational similarity analysis and topological data analysis, reveal matching relational patterns across modalities without explicit mapping; and (ii) methods based on cross-modal embedding alignment, which learn mappings between representation spaces, show that simple linear transformations can bring different embedding spaces into close correspondence, suggesting near-isomorphism. Taken together, the evidence suggests that, even after controlling for trivial commonalities inherent in standard data preprocessing and embedding procedures, a robust structural correspondence persists, hinting at an underlying organizational principle. Some have argued that this result shows that the shared structure is getting at a fundamental, Platonic level of reality. We argue that this conclusion is unjustified. Moreover, we aim to give the idea an alternative philosophical home, rooted in contemporary metasemantics (i.e., theories of what makes a representation and what makes something meaningful) and responses to the symbol grounding problem. We conclude by considering the scope of the RAH and proposing new ways of distinguishing semantic structures that are genuinely invariant from those that inevitably arise due to the fact that all our data is generated under human-specific conditions on Earth.",
      "pdf": "https://arxiv.org/pdf/2602.16584v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16584v1",
      "abs_url": "https://arxiv.org/abs/2602.16584v1",
      "published": "2026-02-18"
    },
    {
      "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS",
      "authors": [
        "Maria Luisa Taccari",
        "Kenza Tazi",
        "Ois\u00edn M. Morrison",
        "Andreas Grafberger",
        "Juan Colonese",
        "Corentin Carton de Wiart",
        "Christel Prudhomme",
        "Cinzia Mazzetti",
        "Matthew Chantry",
        "Florian Pappenberger"
      ],
      "abstract": "Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.",
      "pdf": "https://arxiv.org/pdf/2602.16579v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16579v1",
      "abs_url": "https://arxiv.org/abs/2602.16579v1",
      "published": "2026-02-18"
    },
    {
      "title": "Creating a digital poet",
      "authors": [
        "Vered Tohar",
        "Tsahi Hayat",
        "Amir Leshem"
      ],
      "abstract": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.",
      "pdf": "https://arxiv.org/pdf/2602.16578v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16578v1",
      "abs_url": "https://arxiv.org/abs/2602.16578v1",
      "published": "2026-02-18"
    },
    {
      "title": "MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models",
      "authors": [
        "Antonios Tziorvas",
        "George S. Theodoropoulos",
        "Yannis Theodoridis"
      ],
      "abstract": "Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.",
      "pdf": "https://arxiv.org/pdf/2602.16573v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16573v1",
      "abs_url": "https://arxiv.org/abs/2602.16573v1",
      "published": "2026-02-18"
    },
    {
      "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
      "authors": [
        "Zhuqian Zhou",
        "Kirk Vanacore",
        "Bakhtawar Ahtisham",
        "Jinsook Lee",
        "Doug Pietrzak",
        "Daryl Hedley",
        "Jorge Dias",
        "Chris Shaw",
        "Ruth Sch\u00e4fer",
        "Ren\u00e9 F. Kizilcec"
      ],
      "abstract": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
      "pdf": "https://arxiv.org/pdf/2602.16571v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16571v1",
      "abs_url": "https://arxiv.org/abs/2602.16571v1",
      "published": "2026-02-18"
    },
    {
      "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
      "authors": [
        "Ankur Moitra",
        "Andrej Risteski",
        "Dhruv Rohatgi"
      ],
      "abstract": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).",
      "pdf": "https://arxiv.org/pdf/2602.16570v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16570v1",
      "abs_url": "https://arxiv.org/abs/2602.16570v1",
      "published": "2026-02-18"
    },
    {
      "title": "Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face",
      "authors": [
        "Nicol\u00f2 Di Domenico",
        "Annalisa Franco",
        "Matteo Ferrara",
        "Davide Maltoni"
      ],
      "abstract": "Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.",
      "pdf": "https://arxiv.org/pdf/2602.16569v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16569v1",
      "abs_url": "https://arxiv.org/abs/2602.16569v1",
      "published": "2026-02-18"
    },
    {
      "title": "Separating Oblivious and Adaptive Models of Variable Selection",
      "authors": [
        "Ziyun Chen",
        "Jerry Li",
        "Kevin Tian",
        "Yusong Zhu"
      ],
      "abstract": "Sparse recovery is among the most well-studied problems in learning theory and high-dimensional statistics. In this work, we investigate the statistical and computational landscapes of sparse recovery with $\\ell_\\infty$ error guarantees. This variant of the problem is motivated by \\emph{variable selection} tasks, where the goal is to estimate the support of a $k$-sparse signal in $\\mathbb{R}^d$. Our main contribution is a provable separation between the \\emph{oblivious} (``for each'') and \\emph{adaptive} (``for all'') models of $\\ell_\\infty$ sparse recovery. We show that under an oblivious model, the optimal $\\ell_\\infty$ error is attainable in near-linear time with $\\approx k\\log d$ samples, whereas in an adaptive model, $\\gtrsim k^2$ samples are necessary for any algorithm to achieve this bound. This establishes a surprising contrast with the standard $\\ell_2$ setting, where $\\approx k \\log d$ samples suffice even for adaptive sparse recovery. We conclude with a preliminary examination of a \\emph{partially-adaptive} model, where we show nontrivial variable selection guarantees are possible with $\\approx k\\log d$ measurements.",
      "pdf": "https://arxiv.org/pdf/2602.16568v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16568v1",
      "abs_url": "https://arxiv.org/abs/2602.16568v1",
      "published": "2026-02-18"
    },
    {
      "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
      "authors": [
        "Michael Lanier",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "pdf": "https://arxiv.org/pdf/2602.16564v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16564v1",
      "abs_url": "https://arxiv.org/abs/2602.16564v1",
      "published": "2026-02-18"
    },
    {
      "title": "Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data",
      "authors": [
        "Roya Shomali",
        "Nick Freeman",
        "Greg Bott",
        "Iman Dayarian",
        "Jason Parton"
      ],
      "abstract": "Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.",
      "pdf": "https://arxiv.org/pdf/2602.16561v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16561v1",
      "abs_url": "https://arxiv.org/abs/2602.16561v1",
      "published": "2026-02-18"
    },
    {
      "title": "Illustration of Barren Plateaus in Quantum Computing",
      "authors": [
        "Gerhard Stenzel",
        "Tobias Rohe",
        "Michael K\u00f6lle",
        "Leo S\u00fcnkel",
        "Jonas Stein",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.",
      "pdf": "https://arxiv.org/pdf/2602.16558v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16558v1",
      "abs_url": "https://arxiv.org/abs/2602.16558v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning Distributed Equilibria in Linear-Quadratic Stochastic Differential Games: An $\u03b1$-Potential Approach",
      "authors": [
        "Philipp Plank",
        "Yufei Zhang"
      ],
      "abstract": "We analyze independent policy-gradient (PG) learning in $N$-player linear-quadratic (LQ) stochastic differential games. Each player employs a distributed policy that depends only on its own state and updates the policy independently using the gradient of its own objective. We establish global linear convergence of these methods to an equilibrium by showing that the LQ game admits an $\u03b1$-potential structure, with $\u03b1$ determined by the degree of pairwise interaction asymmetry. For pairwise-symmetric interactions, we construct an affine distributed equilibrium by minimizing the potential function and show that independent PG methods converge globally to this equilibrium, with complexity scaling linearly in the population size and logarithmically in the desired accuracy. For asymmetric interactions, we prove that independent projected PG algorithms converge linearly to an approximate equilibrium, with suboptimality proportional to the degree of asymmetry. Numerical experiments confirm the theoretical results across both symmetric and asymmetric interaction networks.",
      "pdf": "https://arxiv.org/pdf/2602.16555v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16555v1",
      "abs_url": "https://arxiv.org/abs/2602.16555v1",
      "published": "2026-02-18"
    },
    {
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "pdf": "https://arxiv.org/pdf/2602.16554v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16554v1",
      "abs_url": "https://arxiv.org/abs/2602.16554v1",
      "published": "2026-02-18"
    },
    {
      "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
      "authors": [
        "Tianmeng Hu",
        "Yongzheng Cui",
        "Biao Luo",
        "Ke Li"
      ],
      "abstract": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.",
      "pdf": "https://arxiv.org/pdf/2602.16548v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16548v1",
      "abs_url": "https://arxiv.org/abs/2602.16548v1",
      "published": "2026-02-18"
    },
    {
      "title": "Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding",
      "authors": [
        "Kaiting Liu",
        "Hazel Doughty"
      ],
      "abstract": "Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.",
      "pdf": "https://arxiv.org/pdf/2602.16545v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16545v1",
      "abs_url": "https://arxiv.org/abs/2602.16545v1",
      "published": "2026-02-18"
    },
    {
      "title": "Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning",
      "authors": [
        "Jialiang Fan",
        "Shixiong Jiang",
        "Mengyu Liu",
        "Fanxin Kong"
      ],
      "abstract": "Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.",
      "pdf": "https://arxiv.org/pdf/2602.16543v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16543v1",
      "abs_url": "https://arxiv.org/abs/2602.16543v1",
      "published": "2026-02-18"
    },
    {
      "title": "From Latent to Observable Position-Based Click Models in Carousel Interfaces",
      "authors": [
        "Santiago de Leon-Martinez",
        "Robert Moro",
        "Branislav Kveton",
        "Maria Bielikova"
      ],
      "abstract": "Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors.\n  In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation.\n  Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.",
      "pdf": "https://arxiv.org/pdf/2602.16541v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16541v1",
      "abs_url": "https://arxiv.org/abs/2602.16541v1",
      "published": "2026-02-18"
    },
    {
      "title": "Optimal training-conditional regret for online conformal prediction",
      "authors": [
        "Jiadong Liang",
        "Zhimei Ren",
        "Yuxin Chen"
      ],
      "abstract": "We study online conformal prediction for non-stationary data streams subject to unknown distribution drift. While most prior work studied this problem under adversarial settings and/or assessed performance in terms of gaps of time-averaged marginal coverage, we instead evaluate performance through training-conditional cumulative regret. We specifically focus on independently generated data with two types of distribution shift: abrupt change points and smooth drift.\n  When non-conformity score functions are pretrained on an independent dataset, we propose a split-conformal style algorithm that leverages drift detection to adaptively update calibration sets, which provably achieves minimax-optimal regret. When non-conformity scores are instead trained online, we develop a full-conformal style algorithm that again incorporates drift detection to handle non-stationarity; this approach relies on stability - rather than permutation symmetry - of the model-fitting algorithm, which is often better suited to online learning under evolving environments. We establish non-asymptotic regret guarantees for our online full conformal algorithm, which match the minimax lower bound under appropriate restrictions on the prediction sets. Numerical experiments corroborate our theoretical findings.",
      "pdf": "https://arxiv.org/pdf/2602.16537v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16537v1",
      "abs_url": "https://arxiv.org/abs/2602.16537v1",
      "published": "2026-02-18"
    },
    {
      "title": "Exciton-Selective Phonon Coupling in a Lead Halide Perovskite",
      "authors": [
        "Pradeepa H. L.",
        "Sagnik Chatterjee",
        "Sayantan Patra",
        "Swapneswar Bisoi",
        "Saqlain Mushtaq",
        "Hardeep",
        "Akshay Singh",
        "Ashish Arora",
        "Atikur Rahman"
      ],
      "abstract": "Exciton-phonon interactions govern the optical response of semiconductors, yet disentangling multiple coupling channels in lead halide perovskites remains challenging. We investigate CsPbBr3 microcrystals using photoluminescence, Raman and reflectance spectroscopy at low temperature, revealing the simultaneous presence of high-energy and Rashba excitons, each accompanied by distinct phonon replica series. High-energy exciton replicas are uniquely spaced by approximately 9 meV, whereas Rashba exciton replicas exhibit a characteristic approximately 6 meV spacing, indicating the specificity of the exciton-phonon coupling. Unsupervised machine learning applied to a large low-temperature photoluminescence dataset reveals these replica features are prevalent. With increasing temperature, replica features broaden and merge, evolving into a dominant longitudinal optical phonon coupling regime at room temperature. This work establishes direct spectroscopic evidence for concurrent, exciton-specific phonon coupling within a single material, offering new pathways to engineer light-matter interactions for optoelectronic and phonon-photon-based quantum device applications.",
      "pdf": "https://arxiv.org/pdf/2602.16533v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16533v1",
      "abs_url": "https://arxiv.org/abs/2602.16533v1",
      "published": "2026-02-18"
    },
    {
      "title": "Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing",
      "authors": [
        "Daniel Boharon",
        "Yehuda Dar"
      ],
      "abstract": "We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.\n  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.\n  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.",
      "pdf": "https://arxiv.org/pdf/2602.16531v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16531v1",
      "abs_url": "https://arxiv.org/abs/2602.16531v1",
      "published": "2026-02-18"
    },
    {
      "title": "FEKAN: Feature-Enriched Kolmogorov-Arnold Networks",
      "authors": [
        "Sidharth S. Menon",
        "Ameya D. Jagtap"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a compelling alternative to multilayer perceptrons, offering enhanced interpretability via functional decomposition. However, existing KAN architectures, including spline-, wavelet-, radial-basis variants, etc., suffer from high computational cost and slow convergence, limiting scalability and practical applicability. Here, we introduce Feature-Enriched Kolmogorov-Arnold Networks (FEKAN), a simple yet effective extension that preserves all the advantages of KAN while improving computational efficiency and predictive accuracy through feature enrichment, without increasing the number of trainable parameters. By incorporating these additional features, FEKAN accelerates convergence, increases representation capacity, and substantially mitigates the computational overhead characteristic of state-of-the-art KAN architectures. We investigate FEKAN across a comprehensive set of benchmarks, including function-approximation tasks, physics-informed formulations for diverse partial differential equations (PDEs), and neural operator settings that map between input and output function spaces. For function approximation, we systematically compare FEKAN against a broad family of KAN variants, FastKAN, WavKAN, ReLUKAN, HRKAN, ChebyshevKAN, RBFKAN, and the original SplineKAN. Across all tasks, FEKAN demonstrates substantially faster convergence and consistently higher approximation accuracy than the underlying baseline architectures. We also establish the theoretical foundations for FEKAN, showing its superior representation capacity compared to KAN, which contributes to improved accuracy and efficiency.",
      "pdf": "https://arxiv.org/pdf/2602.16530v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16530v1",
      "abs_url": "https://arxiv.org/abs/2602.16530v1",
      "published": "2026-02-18"
    },
    {
      "title": "Capacity-constrained demand response in smart grids using deep reinforcement learning",
      "authors": [
        "Shafagh Abband Pashaki",
        "Sepehr Maleki",
        "Amir Badiee"
      ],
      "abstract": "This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.",
      "pdf": "https://arxiv.org/pdf/2602.16525v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16525v1",
      "abs_url": "https://arxiv.org/abs/2602.16525v1",
      "published": "2026-02-18"
    },
    {
      "title": "Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study",
      "authors": [
        "Gerhard Stenzel",
        "Isabella Debelic",
        "Michael K\u00f6lle",
        "Tobias Rohe",
        "Leo S\u00fcnkel",
        "Julian Hager",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \\(R_x\\), \\(R_y\\), and \\(R_z\\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \\(\u03bb\\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \\(5\\times10^{-4}\\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \\(10^{-4}\\)). Both approaches reliably reconstruct computational basis states (between 83\\% and 99\\% success) and Bell states (between 61\\% and 77\\% success). However, scalability saturates for \\(\u03bb\\) of approximately three to four and does not extend to ten-qubit targets even at \\(\u03bb=2\\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.",
      "pdf": "https://arxiv.org/pdf/2602.16523v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16523v1",
      "abs_url": "https://arxiv.org/abs/2602.16523v1",
      "published": "2026-02-18"
    },
    {
      "title": "Generative deep learning improves reconstruction of global historical climate records",
      "authors": [
        "Zhen Qian",
        "Teng Liu",
        "Sebastian Bathiany",
        "Shangshang Yang",
        "Philipp Hess",
        "Nils Bochow",
        "Christian Burmester",
        "Maximilian Gelbrecht",
        "Brian Groenke",
        "Niklas Boers"
      ],
      "abstract": "Accurate assessment of anthropogenic climate change relies on historical instrumental data, yet observations from the early 20th century are sparse, fragmented, and uncertain. Conventional reconstructions rely on disparate statistical interpolation, which excessively smooths local features and creates unphysical artifacts, leading to systematic underestimation of intrinsic variability and extremes. Here, we present a unified, probabilistic generative deep learning framework that overcomes these limitations and reveals previously unresolved historical climate variability back to 1850. Leveraging a learned generative prior of Earth system dynamics, our model performs probabilistic inference to recover spatiotemporally consistent historical temperature and precipitation fields from sparse observations. Our approach preserves the higher-order statistics of climate dynamics, transforming reconstruction into a robust uncertainty-aware assessment. We demonstrate that our reconstruction overcomes pronounced biases in widely used historical reference products, including those underlying IPCC assessments, especially regarding extreme weather events. Notably, we uncover higher early 20th-century global warming levels compared to existing reconstructions, primarily driven by more pronounced polar warming, with mean Arctic warming trends exceeding established benchmarks by 0.15--0.29\u00b0C per decade for 1900--1980. Conversely, for the modern era, our reconstruction indicates that the broad Arctic warming trend is likely overestimated in recent assessments, yet explicitly resolves previously unrecognized intense, localized hotspots in the Barents Sea and Northeastern Greenland. Furthermore, based on our seamless global reconstruction that recovers precipitation variability across the oceans and under-monitored regions, we uncover an intensification of the global hydrological cycle.",
      "pdf": "https://arxiv.org/pdf/2602.16515v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16515v1",
      "abs_url": "https://arxiv.org/abs/2602.16515v1",
      "published": "2026-02-18"
    },
    {
      "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
      "authors": [
        "Osher Azulay",
        "Zhengjie Xu",
        "Andrew Scheffer",
        "Stella X. Yu"
      ],
      "abstract": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
      "pdf": "https://arxiv.org/pdf/2602.16511v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16511v1",
      "abs_url": "https://arxiv.org/abs/2602.16511v1",
      "published": "2026-02-18"
    },
    {
      "title": "Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?",
      "authors": [
        "Gaetan De Waele",
        "Marek Wydmuch",
        "Krzysztof Dembczy\u0144ski",
        "Wojciech Kot\u0142owski",
        "Willem Waegeman"
      ],
      "abstract": "One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.",
      "pdf": "https://arxiv.org/pdf/2602.16507v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16507v1",
      "abs_url": "https://arxiv.org/abs/2602.16507v1",
      "published": "2026-02-18"
    },
    {
      "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
      "authors": [
        "Sophie Hanna Langbein",
        "Hubert Baniecki",
        "Fabian Fumagalli",
        "Niklas Koenen",
        "Marvin N. Wright",
        "Julia Herbinger"
      ],
      "abstract": "Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.",
      "pdf": "https://arxiv.org/pdf/2602.16505v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16505v1",
      "abs_url": "https://arxiv.org/abs/2602.16505v1",
      "published": "2026-02-18"
    },
    {
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "authors": [
        "Vasilis Gkolemis",
        "Loukas Kavouras",
        "Dimitrios Kyriakopoulos",
        "Konstantinos Tsopelas",
        "Dimitrios Rontogiannis",
        "Giuseppe Casalicchio",
        "Theodore Dalamagas",
        "Christos Diou"
      ],
      "abstract": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.",
      "pdf": "https://arxiv.org/pdf/2602.16503v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16503v1",
      "abs_url": "https://arxiv.org/abs/2602.16503v1",
      "published": "2026-02-18"
    },
    {
      "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
      "authors": [
        "Zhenzhen Huang",
        "Chaoning Zhang",
        "Haoyu Bian",
        "Songbo Zhang",
        "Chi-lok Andy Tai",
        "Jiaquan Zhang",
        "Caiyan Qin",
        "Jingjing Qu",
        "Yalan Ye",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "abstract": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
      "pdf": "https://arxiv.org/pdf/2602.16500v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16500v1",
      "abs_url": "https://arxiv.org/abs/2602.16500v1",
      "published": "2026-02-18"
    },
    {
      "title": "Fast and Scalable Analytical Diffusion",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "abstract": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
      "pdf": "https://arxiv.org/pdf/2602.16498v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16498v1",
      "abs_url": "https://arxiv.org/abs/2602.16498v1",
      "published": "2026-02-18"
    },
    {
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "authors": [
        "Ferdinand Kapl",
        "Emmanouil Angelis",
        "Kaitlin Maile",
        "Johannes von Oswald",
        "Stefan Bauer"
      ],
      "abstract": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
      "pdf": "https://arxiv.org/pdf/2602.16490v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16490v1",
      "abs_url": "https://arxiv.org/abs/2602.16490v1",
      "published": "2026-02-18"
    },
    {
      "title": "Phase-Based Bit Commitment Protocol",
      "authors": [
        "Janis N\u00f6tzel",
        "Anshul Singhal",
        "Peter van Loock"
      ],
      "abstract": "With the rise of artificial intelligence and machine learning, a new wave of private information is being flushed into applications. This development raises privacy concerns, as private datasets can be stolen or abused for non-authorized purposes. Secure function computation aims to solve such problems by allowing a service provider to compute functions of datasets in the possession of a a data provider without reading the data itself. A foundational primitive for such tasks is Bit Commitment (BC), which is known to be impossible to realize without added assumptions. Given the pressing nature of the topic, it is thus important to develop BC systems and prove their security under reasonable assumptions. In this work, we provide a novel quantum optical BC protocol that uses the added assumption that the network provider will secure transmission lines against eavesdropping. Under this added assumption, we prove security of our protocol in the honest but curious setting and discuss the hardness of Mayer's attack in the context of our protocol.",
      "pdf": "https://arxiv.org/pdf/2602.16489v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16489v1",
      "abs_url": "https://arxiv.org/abs/2602.16489v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "authors": [
        "Jonathan Cook",
        "Diego Antognini",
        "Martin Klissarov",
        "Claudiu Musat",
        "Edward Grefenstette"
      ],
      "abstract": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "pdf": "https://arxiv.org/pdf/2602.16488v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16488v1",
      "abs_url": "https://arxiv.org/abs/2602.16488v1",
      "published": "2026-02-18"
    },
    {
      "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data",
      "authors": [
        "Yiwen Lu"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.",
      "pdf": "https://arxiv.org/pdf/2602.16480v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16480v1",
      "abs_url": "https://arxiv.org/abs/2602.16480v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning Preference from Observed Rankings",
      "authors": [
        "Yu-Chang Chen",
        "Chen Chian Fuh",
        "Shang En Tsai"
      ],
      "abstract": "Estimating consumer preferences is central to many problems in economics and marketing. This paper develops a flexible framework for learning individual preferences from partial ranking information by interpreting observed rankings as collections of pairwise comparisons with logistic choice probabilities. We model latent utility as the sum of interpretable product attributes, item fixed effects, and a low-rank user-item factor structure, enabling both interpretability and information sharing across consumers and items. We further correct for selection in which comparisons are observed: a comparison is recorded only if both items enter the consumer's consideration set, inducing exposure bias toward frequently encountered items. We model pair observability as the product of item-level observability propensities and estimate these propensities with a logistic model for the marginal probability that an item is observable. Preference parameters are then estimated by maximizing an inverse-probability-weighted (IPW), ridge-regularized log-likelihood that reweights observed comparisons toward a target comparison population. To scale computation, we propose a stochastic gradient descent (SGD) algorithm based on inverse-probability resampling, which draws comparisons in proportion to their IPW weights. In an application to transaction data from an online wine retailer, the method improves out-of-sample recommendation performance relative to a popularity-based benchmark, with particularly strong gains in predicting purchases of previously unconsumed products.",
      "pdf": "https://arxiv.org/pdf/2602.16476v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16476v1",
      "abs_url": "https://arxiv.org/abs/2602.16476v1",
      "published": "2026-02-18"
    },
    {
      "title": "Certifying Hamilton-Jacobi Reachability Learned via Reinforcement Learning",
      "authors": [
        "Prashant Solanki",
        "Isabelle El-Hajj",
        "Jasper J. van Beers",
        "Erik-Jan van Kampen",
        "Coen C. de Visser"
      ],
      "abstract": "We present a framework to \\emph{certify} Hamilton--Jacobi (HJ) reachability learned by reinforcement learning (RL). Building on a discounted initial time \\emph{travel-cost} formulation that makes small-step RL value iteration provably equivalent to a forward Hamilton--Jacobi (HJ) equation with damping, we convert certified learning errors into calibrated inner/outer enclosures of strict backward reachable tube. The core device is an additive-offset identity: if $W_\u03bb$ solves the discounted travel-cost Hamilton--Jacobi--Bellman (HJB) equation, then $W_\\varepsilon:=W_\u03bb+ \\varepsilon$ solves the same PDE with a constant offset $\u03bb\\varepsilon$. This means that a uniform value error is \\emph{exactly} equal to a constant HJB offset. We establish this uniform value error via two routes: (A) a Bellman operator-residual bound, and (B) a HJB PDE-slack bound. Our framework preserves HJ-level safety semantics and is compatible with deep RL. We demonstrate the approach on a double-integrator system by formally certifying, via satisfiability modulo theories (SMT), a value function learned through reinforcement learning to induce provably correct inner and outer backward-reachable set enclosures over a compact region of interest.",
      "pdf": "https://arxiv.org/pdf/2602.16475v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16475v1",
      "abs_url": "https://arxiv.org/abs/2602.16475v1",
      "published": "2026-02-18"
    },
    {
      "title": "Synthesis and Verification of Transformer Programs",
      "authors": [
        "Hongjian Jiang",
        "Matthew Hague",
        "Philipp R\u00fcmmer",
        "Anthony Widjaja Lin"
      ],
      "abstract": "C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).",
      "pdf": "https://arxiv.org/pdf/2602.16473v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16473v1",
      "abs_url": "https://arxiv.org/abs/2602.16473v1",
      "published": "2026-02-18"
    },
    {
      "title": "Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning",
      "authors": [
        "Jenny Kunz"
      ],
      "abstract": "Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.",
      "pdf": "https://arxiv.org/pdf/2602.16469v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16469v1",
      "abs_url": "https://arxiv.org/abs/2602.16469v1",
      "published": "2026-02-18"
    },
    {
      "title": "HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting",
      "authors": [
        "Jung Min Choi",
        "Vijaya Krishna Yalavarthi",
        "Lars Schmidt-Thieme"
      ],
      "abstract": "In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.",
      "pdf": "https://arxiv.org/pdf/2602.16468v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16468v1",
      "abs_url": "https://arxiv.org/abs/2602.16468v1",
      "published": "2026-02-18"
    },
    {
      "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
      "authors": [
        "Abdulla Jasem Almansoori",
        "Maria Ivanova",
        "Andrey Veprikov",
        "Aleksandr Beznosikov",
        "Samuel Horv\u00e1th",
        "Martin Tak\u00e1\u010d"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.",
      "pdf": "https://arxiv.org/pdf/2602.16456v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16456v1",
      "abs_url": "https://arxiv.org/abs/2602.16456v1",
      "published": "2026-02-18"
    },
    {
      "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
      "authors": [
        "Nicolas Salvy",
        "Hugues Talbot",
        "Bertrand Thirion"
      ],
      "abstract": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.",
      "pdf": "https://arxiv.org/pdf/2602.16449v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16449v1",
      "abs_url": "https://arxiv.org/abs/2602.16449v1",
      "published": "2026-02-18"
    },
    {
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "authors": [
        "Yixue Zhang",
        "Kun Wu",
        "Zhi Gao",
        "Zhen Zhao",
        "Pei Ren",
        "Zhiyuan Xu",
        "Fei Liao",
        "Xinhua Wang",
        "Shichao Fan",
        "Di Wu",
        "Qiuxuan Feng",
        "Meng Li",
        "Zhengping Che",
        "Chang Liu",
        "Jian Tang"
      ],
      "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
      "pdf": "https://arxiv.org/pdf/2602.16444v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16444v1",
      "abs_url": "https://arxiv.org/abs/2602.16444v1",
      "published": "2026-02-18"
    },
    {
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "pdf": "https://arxiv.org/pdf/2602.16442v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16442v1",
      "abs_url": "https://arxiv.org/abs/2602.16442v1",
      "published": "2026-02-18"
    },
    {
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "authors": [
        "Eva Paraschou",
        "Line Harder Clemmensen",
        "Sneha Das"
      ],
      "abstract": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
      "pdf": "https://arxiv.org/pdf/2602.16438v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16438v1",
      "abs_url": "https://arxiv.org/abs/2602.16438v1",
      "published": "2026-02-18"
    },
    {
      "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
      "authors": [
        "Jean Dufraiche",
        "Paul Mangold",
        "Micha\u00ebl Perrot",
        "Marc Tommasi"
      ],
      "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "pdf": "https://arxiv.org/pdf/2602.16436v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16436v1",
      "abs_url": "https://arxiv.org/abs/2602.16436v1",
      "published": "2026-02-18"
    },
    {
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Wangyang Ying",
        "Yanjie Fu"
      ],
      "abstract": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.",
      "pdf": "https://arxiv.org/pdf/2602.16435v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16435v1",
      "abs_url": "https://arxiv.org/abs/2602.16435v1",
      "published": "2026-02-18"
    },
    {
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "abstract": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "pdf": "https://arxiv.org/pdf/2602.16429v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16429v1",
      "abs_url": "https://arxiv.org/abs/2602.16429v1",
      "published": "2026-02-18"
    },
    {
      "title": "Formalized Run-Time Analysis of Active Learning -- Coalgebraically in Agda",
      "authors": [
        "Thorsten Wi\u00dfmann"
      ],
      "abstract": "The objective of automata learning is to reconstruct the implementation of a hidden automaton, to which only a teacher has access. The learner can ask certain kinds of queries to the teacher to gain more knowledge about the hidden automaton. The run-time of such a learning algorithm is then measured in the number of queries it takes until the hidden automaton is successfully reconstructed, which is usually parametric in the number of states of that hidden automaton. How can we prove such a run-time complexity of learning algorithms in a proof assistant if we do not have the hidden automaton and the number of states available?\n  In the present paper, we solve this by considering learning algorithms themselves as generalized automata, more specifically as coalgebras. We introduce formal and yet compact definitions of what a learner and a teacher is, which make it easy to prove upper and lower bounds of different kinds of learning games in the proof assistant Agda.\n  As a running example, we discuss the common number guessing game where a teacher thinks of a natural number and answers guesses by the learner with `correct', `too high', or `too low'. To demonstrate our framework, we formally prove in Agda that both the lower and upper bound on number of guesses by the learner is $\\mathcal{O}(\\log n)$, where $n$ is the teacher's secret number.",
      "pdf": "https://arxiv.org/pdf/2602.16427v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16427v1",
      "abs_url": "https://arxiv.org/abs/2602.16427v1",
      "published": "2026-02-18"
    },
    {
      "title": "Verifiable Semantics for Agent-to-Agent Communication",
      "authors": [
        "Philipp Schoenegger",
        "Matt Carlson",
        "Chris Schneider",
        "Chris Daly"
      ],
      "abstract": "Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms (\"core-guarded reasoning\") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.",
      "pdf": "https://arxiv.org/pdf/2602.16424v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16424v1",
      "abs_url": "https://arxiv.org/abs/2602.16424v1",
      "published": "2026-02-18"
    },
    {
      "title": "Network geometry of the Drosophila brain",
      "authors": [
        "Bendeg\u00faz Sulyok",
        "S\u00e1muel G. Balogh",
        "Gergely Palla"
      ],
      "abstract": "The recent reconstruction of the Drosophila brain provides a neural network of unprecedented size and level of details. In this work, we study the geometrical properties of this system by applying network embedding techniques to the graph of synaptic connections. Since previous analysis have revealed an inhomogeneous degree distribution, we first employ a hyperbolic embedding approach that maps the neural network onto a point cloud in the two-dimensional hyperbolic space. In general, hyperbolic embedding methods exploit the exponentially growing volume of hyperbolic space with increasing distance from the origin, allowing for an approximately uniform spatial distribution of nodes even in scale-free, small-world networks. By evaluating multiple embedding quality metrics, we find that the network structure is well captured by the resulting two-dimensional hyperbolic embedding, and in fact is more congruent with this representation than with the original neuron coordinates in three-dimensional Euclidean space. In order to examine the network geometry in a broader context, we also apply the well-known Euclidean network embedding approach Node2vec, where the dimension of the embedding space, $d$ can be set arbitrarily. In 3 dimensions, the Euclidean embedding of the network yields lower quality scores compared to the original neuron coordinates. However, as a function of the embedding dimension the scores show an improving tendency, surpassing the level of the 2d hyperbolic embedding roughly at $d=16$, and reaching a maximum around $d=64$. Since network embeddings can serve as valuable inputs for a variety of downstream machine learning tasks, our results offer new perspectives on the structure and representation of this recently revealed and biologically significant neural network.",
      "pdf": "https://arxiv.org/pdf/2602.16417v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16417v1",
      "abs_url": "https://arxiv.org/abs/2602.16417v1",
      "published": "2026-02-18"
    },
    {
      "title": "The Astronomical Telescope of the University of Stuttgart (ATUS): Development, Optimization, and Lessons Learned",
      "authors": [
        "Karsten Schindler",
        "J\u00fcrgen Wolf",
        "Alfred Krabbe"
      ],
      "abstract": "ATUS, the Astronomical Telescope of the University of Stuttgart, is a fully remote-controlled 0.6 m f/8.17 Ritchey-Chr\u00e9tien telescope optimized for high-cadence, high-fidelity photometry of transient sources. Observations are time-referenced with very high accuracy and precision, making it an ideal platform for time-domain astronomy and space situational awareness. Initially conceived to support instrument developments and operations of SOFIA, the Stratospheric Observatory for Infrared Astronomy, it evolved into a scientific instrument for various use cases in instrument development, astronomical research, and teaching. This paper presents an overview of its development and optimization to achieve diffraction-limited images and highly accurate pointing and tracking, even at high speeds. The findings and lessons learned are universally applicable to other telescopes that are currently at the planning stage, or where similar issues might be encountered.",
      "pdf": "https://arxiv.org/pdf/2602.16415v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16415v1",
      "abs_url": "https://arxiv.org/abs/2602.16415v1",
      "published": "2026-02-18"
    },
    {
      "title": "Easy Data Unlearning Bench",
      "authors": [
        "Roy Rinberg",
        "Pol Puigdemont",
        "Martin Pawelczyk",
        "Volkan Cevher"
      ],
      "abstract": "Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.",
      "pdf": "https://arxiv.org/pdf/2602.16400v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16400v1",
      "abs_url": "https://arxiv.org/abs/2602.16400v1",
      "published": "2026-02-18"
    },
    {
      "title": "Multi-Channel Replay Speech Detection using Acoustic Maps",
      "authors": [
        "Michael Neri",
        "Tuomas Virtanen"
      ],
      "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.",
      "pdf": "https://arxiv.org/pdf/2602.16399v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16399v1",
      "abs_url": "https://arxiv.org/abs/2602.16399v1",
      "published": "2026-02-18"
    },
    {
      "title": "Variable-Length Semantic IDs for Recommender Systems",
      "authors": [
        "Kirill Khrylchenko"
      ],
      "abstract": "Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.\n  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.\n  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.",
      "pdf": "https://arxiv.org/pdf/2602.16375v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16375v1",
      "abs_url": "https://arxiv.org/abs/2602.16375v1",
      "published": "2026-02-18"
    },
    {
      "title": "Improved Bounds for Reward-Agnostic and Reward-Free Exploration",
      "authors": [
        "Oran Ridel",
        "Alon Cohen"
      ],
      "abstract": "We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $\u03b5$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $\u03b5$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $\u03b5$. We propose a new algorithm that significantly relaxes the requirement on $\u03b5$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $\u03b5$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.",
      "pdf": "https://arxiv.org/pdf/2602.16363v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16363v1",
      "abs_url": "https://arxiv.org/abs/2602.16363v1",
      "published": "2026-02-18"
    },
    {
      "title": "Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks",
      "authors": [
        "Sarkis Ter Martirosyan",
        "Xinyue Huang",
        "David Qin",
        "Anthony Yu",
        "Stanislav Emelianov"
      ],
      "abstract": "Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \\textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.",
      "pdf": "https://arxiv.org/pdf/2602.16357v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16357v1",
      "abs_url": "https://arxiv.org/abs/2602.16357v1",
      "published": "2026-02-18"
    },
    {
      "title": "An assortment of problems in permutation patterns: unimodality, equivalence, derangements, and sorting",
      "authors": [
        "Vincent Vatter"
      ],
      "abstract": "We collect open problems in permutation patterns on four themes: rank-unimodality in the permutation pattern poset, Wilf-equivalence and shape-Wilf-equivalence, the enumeration of derangements in permutation classes, and sorting by stacks in series, generalized stacks, and restricted containers (C-machines).",
      "pdf": "https://arxiv.org/pdf/2602.16355v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16355v1",
      "abs_url": "https://arxiv.org/abs/2602.16355v1",
      "published": "2026-02-18"
    },
    {
      "title": "Dual-Quadruped Collaborative Transportation in Narrow Environments via Safe Reinforcement Learning",
      "authors": [
        "Zhezhi Lei",
        "Zhihai Bi",
        "Wenxin Wang",
        "Jun Ma"
      ],
      "abstract": "Collaborative transportation, where multiple robots collaboratively transport a payload, has garnered significant attention in recent years. While ensuring safe and high-performance inter-robot collaboration is critical for effective task execution, it is difficult to pursue in narrow environments where the feasible region is extremely limited. To address this challenge, we propose a novel approach for dual-quadruped collaborative transportation via safe reinforcement learning (RL). Specifically, we model the task as a fully cooperative constrained Markov game, where collision avoidance is formulated as constraints. We introduce a cost-advantage decomposition method that enforces the sum of team constraints to remain below an upper bound, thereby guaranteeing task safety within an RL framework. Furthermore, we propose a constraint allocation method that assigns shared constraints to individual robots to maximize the overall task reward, encouraging autonomous task-assignment among robots, thereby improving collaborative task performance. Simulation and real-time experimental results demonstrate that the proposed approach achieves superior performance and a higher success rate in dual-quadruped collaborative transportation compared to existing methods.",
      "pdf": "https://arxiv.org/pdf/2602.16353v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16353v1",
      "abs_url": "https://arxiv.org/abs/2602.16353v1",
      "published": "2026-02-18"
    }
  ]
}